{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import scrape\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads from csv file of all states and creates list of lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska', 'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'vermont', 'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T430s\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Use Pandas to read data\n",
    "states_pd = pd.read_csv(\"states.csv\", \"utf-8\")\n",
    "\n",
    "# Convert df to list\n",
    "states = states_pd['State_Name'].values.tolist()\n",
    "\n",
    "# Convert list to lowercase values\n",
    "states_lower = []\n",
    "for state in states:\n",
    "    states_lower.append(state.lower())\n",
    "    \n",
    "print(states_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a state name as a string, appends it to the base url, and based on the new url <br> \n",
    "it scrapes usnews.com's list of all states by ranking. Output is a dictionary of state stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(state):\n",
    "\n",
    "    # Initialize base url\n",
    "    base_url = \"https://www.usnews.com/news/best-states/\"\n",
    "\n",
    "    # create final url using \"state\"\n",
    "    state_scraped = scrape(base_url + state)\n",
    "\n",
    "    # Save div as object to scrape & loop through to find all elements of state stats\n",
    "    div = state_scraped.find(\"div\", class_=\"Grid-s1x0i6w9-0\").find_all(\"dl\", class_=\"QuickStats-s9hlu1u-0\")\n",
    "\n",
    "    # Initialize empty list to save values\n",
    "    list = []\n",
    "\n",
    "    for d in div[:3]:\n",
    "        for i in d:\n",
    "            list.append(i.text)\n",
    "\n",
    "    # Skip every other item in list to append to values\n",
    "    category = (list[::2])\n",
    "    cat_value = (list[1::2])   \n",
    "\n",
    "    # Zip values into dictionary for stats\n",
    "    quick_stats_dict = dict(zip(category, cat_value))\n",
    "\n",
    "    # Save new div as object to scrape & find rankings\n",
    "    rankings = state_scraped.find(\"div\", class_=\"Cell-s1jgw6rh-0\")\n",
    "    rank_list = rankings.find(\"ul\")\n",
    "\n",
    "    # Save new div as object to scrape & find overall rating\n",
    "    overall_rating  = rankings.find_all(\"div\", class_=\"DonutMeter__Wrapper-s1jo49pn-0 gNXJuS\")[0].text\n",
    "\n",
    "    # Save as integer to be calculated\n",
    "    overall_rating_num = int(overall_rating.replace(\"#\",\"\"))\n",
    "\n",
    "    ranking_title = []\n",
    "    rankings = []\n",
    "\n",
    "    for list in rank_list:\n",
    "        ranking_title.append(list.find(\"span\").text)\n",
    "        rankings.append(list.find(\"b\").text)\n",
    "\n",
    "    # Must remove # from each number & convert to int to perform calculations\n",
    "    new_rankings = []\n",
    "\n",
    "    for rank in rankings:\n",
    "        new_rankings.append(int(rank.replace(\"#\",\"\")))\n",
    "\n",
    "    # Calculate \"rank\" into \"percentage\" \n",
    "    perc_rankings = []\n",
    "    for rank in new_rankings:\n",
    "        perc_rankings.append((1 - rank/50)*100)\n",
    "\n",
    "    # Save ranking values into dataframe\n",
    "    rankings_df = pd.DataFrame({'Category': ranking_title,\n",
    "                                'Rank' : rankings,\n",
    "                                'Value' : new_rankings,\n",
    "                                'Percentage': perc_rankings\n",
    "                               })\n",
    "\n",
    "    # Convert to dictionary\n",
    "    rankings_dict = rankings_df.to_dict('list')\n",
    "    \n",
    "    # Turn dataframe into list of dictionaries per tuple\n",
    "    new_df = rankings_df.to_dict(orient='records')[1:-1]\n",
    "    \n",
    "    # Find URL of state image\n",
    "    image_url = state_scraped.find(\"div\", class_=\"Profile__ProfileWakanda-h5rw0b-2\")\\\n",
    "    .find(\"div\", class_= \"s85n6m5-0-Box-cwadsP cBeNoA\")\\\n",
    "    .find(\"picture\", class_=\"Image__Picture-afx55j-0\").find(\"img\")[\"src\"]\n",
    "\n",
    "    new_state = state.replace(\"-\", \" \")\n",
    "    \n",
    "    # Create new dictionary from all variables\n",
    "    state_dict = {\n",
    "        \"Image URL\": image_url,\n",
    "        \"Overall Rating\": overall_rating,\n",
    "        \"Overall Rating Number\": overall_rating_num,\n",
    "        \"State\": new_state,\n",
    "        \"Stats Dictionary\": quick_stats_dict,\n",
    "        \"Rank Dictionary\": new_df \n",
    "    }\n",
    "    \n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created state list \n",
    "state_list = ['arizona', 'california', 'colorado', 'illinois', 'kentucky', 'florida', 'georgia',\\\n",
    "              'massachusetts', 'michigan', 'minnesota',\\\n",
    "              'new-york', 'north-carolina', 'ohio', 'south-carolina', 'tennessee', 'texas', \\\n",
    "              'virginia', 'washington']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to scrape multiple states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a LIST of states, and loops through, calling the state scraper function to scrape <br>\n",
    "each state. From the output, it creates a list of dictionaries for each state and outputs a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_states(list_states):\n",
    "    \n",
    "    all_states_dict = []\n",
    "\n",
    "    for state in list_states:\n",
    "        try:\n",
    "            state_dict = get_states(state)\n",
    "            case = {state: state_dict}\n",
    "            all_states_dict.append(case)\n",
    "        except:\n",
    "            print(f'An error occurred for state ' + state)\n",
    "    \n",
    "    # Turn dictionary into JSON file\n",
    "    with open('states_results.json', 'w') as fp:\n",
    "        json.dump(all_states_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for state california\n",
      "An error occurred for state colorado\n",
      "An error occurred for state illinois\n",
      "An error occurred for state kentucky\n",
      "An error occurred for state florida\n",
      "An error occurred for state georgia\n",
      "An error occurred for state massachusetts\n",
      "An error occurred for state michigan\n",
      "An error occurred for state minnesota\n",
      "An error occurred for state new-york\n",
      "An error occurred for state north-carolina\n",
      "An error occurred for state ohio\n",
      "An error occurred for state south-carolina\n",
      "An error occurred for state tennessee\n",
      "An error occurred for state texas\n",
      "An error occurred for state virginia\n",
      "An error occurred for state washington\n"
     ]
    }
   ],
   "source": [
    "combine_states(state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.usnews.com/dims4/USNEWS/0a65959/2147483647/crop/4050x1895%2B0%2B802/resize/1000x468/quality/85/?url=http%3A%2F%2Fcom-usnews-beam-media.s3.amazonaws.com%2F49%2F3f%2F614945704eafae7e1a9cf59366bf%2Fbs19-washington-editorial.jpg\n"
     ]
    }
   ],
   "source": [
    "pic_containter = test_scraped.find(\"div\", class_=\"Profile__ProfileWakanda-h5rw0b-2\")\\\n",
    ".find(\"div\", class_= \"s85n6m5-0-Box-cwadsP cBeNoA\")\\\n",
    ".find(\"picture\", class_=\"Image__Picture-afx55j-0\").find(\"img\")[\"src\"]\n",
    "print(pic_containter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
